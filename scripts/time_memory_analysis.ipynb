{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import regex\n",
    "\n",
    "from collections import (\n",
    "    namedtuple,\n",
    "    defaultdict,\n",
    ")\n",
    "\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/dimitriy/Effect_Healthcare/TS-Mapper-DataStructures/output\n"
     ]
    }
   ],
   "source": [
    "NODE_SIZES = [1, 10, 100, 1000, 10000, 100000, 1000000]\n",
    "\n",
    "OUTPUT_DIR = Path(Path.cwd() / \"..\" / \"output\" ).resolve()\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "PLOT_DIR = Path(OUTPUT_DIR / \"plots\" )\n",
    "PLOT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "BENCHMARK_JSON = Path(OUTPUT_DIR / \"benchmark.report.json\") # Memory Analysis\n",
    "\n",
    "ReportEntry = namedtuple(\"ReportEntry\", [\"spec\", \"measurement\", \"entry_value\"])\n",
    "\n",
    "\n",
    "print(OUTPUT_DIR.resolve())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "278\n",
      "279\n"
     ]
    }
   ],
   "source": [
    "def load_json_report(fp: Path):\n",
    "    with open(fp, \"r\") as f:\n",
    "        data = json.load(fp.open())\n",
    "    return data\n",
    "\n",
    "benchmarks_data = load_json_report(BENCHMARK_JSON)\n",
    "\n",
    "\n",
    "# Filter the nodes to extract those whose measurement is \"performance\"\n",
    "\n",
    "def extract(benchmark):\n",
    "    if \"measurement\" not in benchmark:\n",
    "        raise ValueError(\"The benchmark does not have a measurement key\")\n",
    "    \n",
    "    spec = benchmark.get(\"spec\")\n",
    "    measurement_dict = benchmark.get(\"measurement\") # Get the measurement dictionary\n",
    "    benchmark_type = list(measurement_dict.keys())[0] # Get the type of the benchmark (memory or performance)\n",
    "    recorded_value_dict = measurement_dict.get(benchmark_type) # Get the value of the measurement\n",
    "    entry_value = recorded_value_dict.get(\"time\") if benchmark_type == \"performance\" else recorded_value_dict.get(\"size\")    \n",
    "    return ReportEntry(\n",
    "        spec=spec,\n",
    "        measurement=benchmark_type,\n",
    "        entry_value=entry_value\n",
    "    )\n",
    "    \n",
    "\n",
    "performance_benchmarks = list(\n",
    "    map(extract, filter(lambda x: \"performance\" in x[\"measurement\"], benchmarks_data))\n",
    ")\n",
    "memory_benchmarks = list(\n",
    "    map(extract, filter(lambda x:  \"memory\" in x[\"measurement\"], benchmarks_data))\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "print(len(performance_benchmarks))\n",
    "print(len(memory_benchmarks))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "140\n",
      "139\n",
      "139\n",
      "139\n"
     ]
    }
   ],
   "source": [
    "# Split the benchmark specs based on the data structure used\n",
    "# The spec description is in the form of \"Given <Object|Tuple> Data Structure...\"\n",
    "\n",
    "def split_benchmarks(benchmarks):\n",
    "    object_benchmarks = list(filter(lambda x: \"Object\" in x.spec, benchmarks))\n",
    "    tuple_benchmarks = list(filter(lambda x: \"Tuple\" in x.spec, benchmarks))\n",
    "    return object_benchmarks, tuple_benchmarks\n",
    "\n",
    "object_performance_benchmarks, tuple_performance_benchmarks = split_benchmarks(performance_benchmarks)\n",
    "object_memory_benchmarks, tuple_memory_benchmarks = split_benchmarks(memory_benchmarks)\n",
    "\n",
    "print(len(object_memory_benchmarks))\n",
    "print(len(tuple_performance_benchmarks))\n",
    "\n",
    "print(len(object_performance_benchmarks))\n",
    "print(len(tuple_memory_benchmarks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 10\n",
      "10 10\n",
      "100 10\n",
      "1000 10\n",
      "10000 10\n",
      "100000 10\n",
      "1000000 10\n"
     ]
    }
   ],
   "source": [
    "def create_size_dict(benchmarks):\n",
    "    size_dict = defaultdict(list)\n",
    "    for benchmark in benchmarks:\n",
    "        size = int(regex.search(r\"\\d+\", benchmark.spec).group())\n",
    "        size_dict[size].append(benchmark)\n",
    "    return size_dict\n",
    "\n",
    "def sort_benchmarks(benchmarks):\n",
    "    #benchmarks.sort(key=lambda x: int(regex.search(r\"\\d+\", x.spec).group()))\n",
    "    return sorted(benchmarks, key=lambda x: int(regex.search(r\"\\d+\", x.spec).group()))\n",
    "\n",
    "def split_on_details(benchmarks):\n",
    "    # Split on if the word small or large is in the spec\n",
    "    small_benchmarks = list(filter(lambda x: \"small\" in x.spec and \"large\" not in x.spec, benchmarks))\n",
    "    large_benchmarks = list(filter(lambda x: \"large\" in x.spec and \"small\" not in x.spec, benchmarks))\n",
    "\n",
    "    small_benchmarks, large_benchmarks= sort_benchmarks(small_benchmarks), sort_benchmarks(large_benchmarks)\n",
    "    \n",
    "    # Create a dictionary of the benchmarks where the key is the size of the data structure and the values are the benchmarks\n",
    "    return create_size_dict(small_benchmarks), create_size_dict(large_benchmarks)\n",
    "\n",
    "    #return small_benchmarks, large_benchmarks\n",
    "            \n",
    "object_small_performance_benchmarks, object_large_performance_benchmarks = split_on_details(object_performance_benchmarks)\n",
    "object_small_memory_benchmarks, object_large_memory_benchmarks = split_on_details(object_memory_benchmarks)\n",
    "\n",
    "tuple_small_performance_benchmarks, tuple_large_performance_benchmarks = split_on_details(tuple_performance_benchmarks)\n",
    "tuple_small_memory_benchmarks, tuple_large_memory_benchmarks = split_on_details(tuple_memory_benchmarks)\n",
    "\n",
    "for key, value in object_large_memory_benchmarks.items():\n",
    "    print(key, len(value))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data_structure</th>\n",
       "      <th>node_size</th>\n",
       "      <th>measurement</th>\n",
       "      <th>time_(s)</th>\n",
       "      <th>time_(s)_max_scaled</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Tuple</td>\n",
       "      <td>1</td>\n",
       "      <td>performance</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.000033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Tuple</td>\n",
       "      <td>1</td>\n",
       "      <td>performance</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>0.000066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Tuple</td>\n",
       "      <td>1</td>\n",
       "      <td>performance</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>0.000066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Tuple</td>\n",
       "      <td>1</td>\n",
       "      <td>performance</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Tuple</td>\n",
       "      <td>1</td>\n",
       "      <td>performance</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.000033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>Tuple</td>\n",
       "      <td>1000000</td>\n",
       "      <td>performance</td>\n",
       "      <td>0.0288</td>\n",
       "      <td>0.009476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>Tuple</td>\n",
       "      <td>1000000</td>\n",
       "      <td>performance</td>\n",
       "      <td>0.5301</td>\n",
       "      <td>0.174421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>Tuple</td>\n",
       "      <td>1000000</td>\n",
       "      <td>performance</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>Tuple</td>\n",
       "      <td>1000000</td>\n",
       "      <td>performance</td>\n",
       "      <td>0.0359</td>\n",
       "      <td>0.011812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>Tuple</td>\n",
       "      <td>1000000</td>\n",
       "      <td>performance</td>\n",
       "      <td>0.6437</td>\n",
       "      <td>0.211799</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>70 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   data_structure  node_size  measurement  time_(s)  time_(s)_max_scaled\n",
       "0           Tuple          1  performance    0.0001             0.000033\n",
       "1           Tuple          1  performance    0.0002             0.000066\n",
       "2           Tuple          1  performance    0.0002             0.000066\n",
       "3           Tuple          1  performance    0.0000             0.000000\n",
       "4           Tuple          1  performance    0.0001             0.000033\n",
       "..            ...        ...          ...       ...                  ...\n",
       "65          Tuple    1000000  performance    0.0288             0.009476\n",
       "66          Tuple    1000000  performance    0.5301             0.174421\n",
       "67          Tuple    1000000  performance    0.0000             0.000000\n",
       "68          Tuple    1000000  performance    0.0359             0.011812\n",
       "69          Tuple    1000000  performance    0.6437             0.211799\n",
       "\n",
       "[70 rows x 5 columns]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def create_dataframe(benchmarks_dict):\n",
    "    data = []    # Sort the benchmarks based on the size of the data structure\n",
    "\n",
    "    for size, benchmarks in benchmarks_dict.items():\n",
    "        for benchmark in benchmarks:\n",
    "            recorded_value_name = \"time_(ms)\" if benchmark.measurement == \"performance\" else \"size_(bytes)\"\n",
    "            recorded_value = benchmark.entry_value\n",
    "\n",
    "            # Convert the recorded value to seconds if the measurement is performance\n",
    "            if benchmark.measurement == \"performance\":\n",
    "                recorded_value_name = \"time_(s)\"\n",
    "                recorded_value = recorded_value / 1000\n",
    "\n",
    "            data.append({\n",
    "                \"data_structure\": \"Object\" if \"Object\" in benchmark.spec else \"Tuple\",\n",
    "                \"node_size\": size,\n",
    "                \"measurement\": benchmark.measurement,\n",
    "                recorded_value_name: recorded_value,\n",
    "            })\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    #df = df.sort_values(by=[\"node_size\", \"measurement\"])\n",
    "    # Max-scale the recorded values by dividing by the maximum value in the column\n",
    "    if \"time_(s)\" in df.columns:\n",
    "        df[\"time_(s)_max_scaled\"] = df[\"time_(s)\"] / df[\"time_(s)\"].max()\n",
    "\n",
    "    if \"size (bytes)\" in df.columns:\n",
    "        df[\"size (bytes)_max_scaled\"] = df[\"size_(bytes)\"] / df[\"size_(bytes)\"].max()\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "object_small_performance_df = create_dataframe(object_small_performance_benchmarks)\n",
    "object_large_performance_df = create_dataframe(object_large_performance_benchmarks)\n",
    "\n",
    "object_small_memory_df = create_dataframe(object_small_memory_benchmarks)\n",
    "object_large_memory_df = create_dataframe(object_large_memory_benchmarks)\n",
    "\n",
    "tuple_small_performance_df = create_dataframe(tuple_small_performance_benchmarks)\n",
    "tuple_large_performance_df = create_dataframe(tuple_large_performance_benchmarks)\n",
    "\n",
    "tuple_small_memory_df = create_dataframe(tuple_small_memory_benchmarks)\n",
    "tuple_large_memory_df = create_dataframe(tuple_large_memory_benchmarks)\n",
    "\n",
    "memory_dfs = [object_small_memory_df, object_large_memory_df, tuple_small_memory_df, tuple_large_memory_df]\n",
    "performance_dfs = [object_small_performance_df, object_large_performance_df, tuple_small_performance_df, tuple_large_performance_df]\n",
    "object_dfs = [object_small_memory_df, object_large_memory_df, object_small_performance_df, object_large_performance_df]\n",
    "tuple_dfs = [tuple_small_memory_df, tuple_large_memory_df, tuple_small_performance_df, tuple_large_performance_df]\n",
    "\n",
    "def plot_memory(df, title, output_fp):\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.set_style(\"whitegrid\")\n",
    "    sns.lineplot(x=\"node_size\", y=\"size_(bytes)\", hue=\"data_structure\", style=\"measurement\", markers=True, data=df, )\n",
    "    plt.title(title)\n",
    "    plt.savefig(output_fp)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "# Plot the memory benchmarks against the node size for both the object and tuple data structures\n",
    "\n",
    "#plot_memory(object_small_memory_df, \"Memory Usage vs Node Size (Object Data Structure)\", PLOT_DIR / \"memory_object_small.png\")\n",
    "\n",
    "\n",
    "\n",
    "tuple_large_performance_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nNumber of Graphs:\\n    - 1 per operation (3*3 + 1) = 10 (Assuming performance is Y1 and memory is Y2)\\n        - Else: 20 graphs, but only 1 Y axis \\n\\n\\nGeneric Graph:\\n- Axes:\\n    - Y:\\n        - Y1: Performance (Time in seconds)\\n        - Y2: Memory (Size in bytes)\\n    - X: Node Size (1, 10, 100, 1000, 10000, 100000, 1000000)\\n\\n- Data (Series 1 => Small | Series 2 => Large):\\n    - Data Point:\\n\\nWhat we want to see:\\n    - Difference in performance between the object and tuple data structures\\n    - \\n\\n\\n'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# The raw values of the benchmarks are not very useful for visualization, so we will normalize the values by dividing them by the maximum value in the data frame\n",
    "\n",
    "\"\"\"\n",
    "Number of Graphs:\n",
    "    - 1 per operation (3*3 + 1) = 10 (Assuming performance is Y1 and memory is Y2)\n",
    "        - Else: 20 graphs, but only 1 Y axis \n",
    "\n",
    "\n",
    "Generic Graph:\n",
    "- Axes:\n",
    "    - Y:\n",
    "        - Y1: Performance (Time in seconds)\n",
    "        - Y2: Memory (Size in bytes)\n",
    "    - X: Node Size (1, 10, 100, 1000, 10000, 100000, 1000000)\n",
    "\n",
    "- Data (Series 1 => Small | Series 2 => Large):\n",
    "    - Data Point:\n",
    "\n",
    "What we want to see:\n",
    "    - Difference in performance between the object and tuple data structures\n",
    "    - \n",
    "\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
